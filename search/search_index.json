{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"youtube-to-docs","text":"<p>Click on the image below to see a demo of YouTube to docs:</p> <p>Convert YouTube videos into structured docs, summaries, audio, and visual assets for easier discovery.</p> <p>View all available CLI options:</p> <pre><code>uvx youtube-to-docs --help\n</code></pre>"},{"location":"#optional-features","title":"Optional Features","text":"<p>To keep the installation light, some features are optional. You can enable them by specifying \"extras\":</p> <ul> <li><code>audio</code>: Required for TTS and audio processing (uses <code>yt-dlp</code>).</li> <li><code>video</code>: Required for video generation (uses <code>static-ffmpeg</code>).</li> <li><code>workspace</code>: Required for Google Drive integration.</li> <li><code>m365</code>: Required for Microsoft SharePoint/OneDrive integration.</li> <li><code>aws</code>: AWS Bedrock support.</li> <li><code>azure</code>: Required for Azure OpenAI models.</li> <li><code>gcp</code>: Required for Google Gemini, Vertex AI models, and GCP Cloud TTS (uses <code>google-genai</code> and google-cloud libs).</li> <li><code>all</code>: Installs everything.</li> </ul> <p>Example: Run with audio and video support <pre><code>uvx --with \"youtube-to-docs[audio,video]\" youtube-to-docs ...\n</code></pre></p> <p>Example: Run with everything <pre><code>uvx --with \"youtube-to-docs[all]\" youtube-to-docs ...\n</code></pre></p> <p>Note: The commands above require <code>uv</code>. You can install it via: *   macOS/Linux: <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code> *   Windows: <code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"</code></p> <p>Install as a Gemini CLI extension:</p> <pre><code>gemini extensions install https://github.com/DoIT-Artificial-Intelligence/youtube-to-docs.git\n</code></pre> <p>Created with the help of AI. All artifacts have been checked and work as expected.</p>"},{"location":"how-this-works/","title":"How This Works","text":"<p><code>youtube-to-docs</code> automates the conversion of YouTube videos into structured documentation and multimodal assets. This document explains the technical workflow and core components of the system.</p>"},{"location":"how-this-works/#system-architecture","title":"System Architecture","text":"<p>The tool operates as a pipeline that ingests YouTube content, processes it through various AI models, and outputs structured data and files.</p>"},{"location":"how-this-works/#1-input-resolution","title":"1. Input Resolution","text":"<p>The entry point (<code>youtube_to_docs/main.py</code>) accepts flexible inputs: - Video IDs: Direct processing. - Playlists: Resolves all video IDs within a playlist. - Channels: Resolves a channel's \"Uploads\" playlist to process all their videos.</p> <p>Key Component: <code>youtube_to_docs.transcript.resolve_video_ids</code> uses the YouTube Data API to fetch lists of videos when a Playlist or Channel is provided.</p> <pre><code>*   **AI Source**: If specified, an AI model (like Gemini 3 Flash) processes the extracted audio file to generate a fresh, potentially higher-accuracy transcript.\n*   **SRT Generation**: For both YouTube and AI sources, the system generates an `.srt` file. This is crucial for accessibility and provides the raw timing data used for precision Q&amp;A alignment.\n</code></pre> <p>Note on Auto-Captions: Automatic captions are generated by speech recognition and may have accuracy issues. They are not always immediately available.</p>"},{"location":"how-this-works/#3-the-llm-pipeline","title":"3. The LLM Pipeline","text":"<p>Text processing is handled by Large Language Models (LLMs) defined in <code>youtube_to_docs/llms.py</code>. The pipeline is model-agnostic, supporting Google Gemini, Vertex AI, AWS Bedrock, and Azure Foundry.</p> <p>For each video, the specified model performs three distinct tasks:</p> <ol> <li> <p>Speaker Extraction:</p> <ul> <li>Input: Full transcript.</li> <li>Task: Identify speakers and their professional titles/roles.</li> <li>Output: A structured list (e.g., \"Speaker 1 (Host)\").</li> </ul> </li> <li> <p>Q&amp;A Generation:</p> <ul> <li>Input: Full transcript + Identified Speakers + Timing Reference (SRT).</li> <li>Task: Extract key questions and answers discussed in the video.</li> <li>Precision Timing: If YouTube SRT is available, it is passed to the LLM as a \"Timing Reference\". The model uses this to align the high-quality speech identification of the AI transcript with the pinpoint accuracy of YouTube's timing.</li> <li>Output: A Markdown table with columns for Questioner, Question, Responder, Answer, Timestamp, and Timestamp URL (formatted as a markdown hyperlink).</li> </ul> </li> <li> <p>Summarization:</p> <ul> <li>Input: Full transcript + Video Metadata.</li> <li>Task: Create a concise, comprehensive summary of the content.</li> <li>Output: A Markdown-formatted summary.</li> </ul> </li> <li> <p>Tag Generation:</p> <ul> <li>Input: Full transcript.</li> <li>Task: Generate up to 5 comma-separated tags for the transcript.</li> <li>Output: A comma-separated string of tags.</li> </ul> </li> <li> <p>Multi-Language Support:</p> <ul> <li>The tool supports processing videos in multiple languages via the <code>--language</code> argument.</li> <li>It iterates through each requested language, fetching or generating transcripts, summaries, Q&amp;A, and infographics for that specific language.</li> <li>File names and column headers are suffixed with the language code (e.g., <code>(es)</code>) to keep assets organized.</li> </ul> </li> </ol>"},{"location":"how-this-works/#5-multimodal-generation","title":"5. Multimodal Generation","text":"<p>Beyond text, the tool creates audio and visual assets:</p> <ul> <li> <p>Text-to-Speech (TTS):</p> <ul> <li>Uses models (like Gemini's TTS) to convert the generated summary into an audio file.</li> <li>This allows users to \"listen\" to the video summary.</li> </ul> </li> <li> <p>Infographics:</p> <ul> <li>Uses image generation models to create a visual representation of the summary.</li> <li>Supported Providers:<ul> <li>Google: Gemini, Imagen.</li> <li>AWS Bedrock: Titan Image Generator, Nova Canvas (requires <code>AWS_BEARER_TOKEN_BEDROCK</code>).</li> <li>Azure Foundry: GPT Image models (requires <code>AZURE_FOUNDRY_ENDPOINT</code> and <code>AZURE_FOUNDRY_API_KEY</code>).</li> </ul> </li> <li>The prompt includes the video title and the generated summary text to ensure relevance.</li> </ul> </li> <li> <p>Multimodal Alt Text:</p> <ul> <li>Once an infographic is generated, an AI model processes the image bytes directly to generate descriptive alt text.</li> <li>Post-processing: The tool automatically strips common prefixes like \"Alt text: \" to ensure the output is clean and ready for accessibility use.</li> <li>This ensures infographics are accessible and searchable.</li> </ul> </li> <li> <p>Video Generation:</p> <ul> <li>Combines the generated infographic (visual) and TTS audio (sound) into a single MP4 video file.</li> <li>Uses <code>static-ffmpeg</code> to perform the merging, ensuring no external FFmpeg installation is required.</li> <li>This provides a shareable \"video summary\" format.</li> </ul> </li> </ul>"},{"location":"how-this-works/#6-cost-tracking","title":"6. Cost Tracking","text":"<p>The system includes a pricing engine (<code>youtube_to_docs/prices.py</code>) that tracks token usage for every API call. - It calculates costs for input and output tokens based on the specific model used. - Costs are aggregated for speaker extraction, Q&amp;A, summarization, and infographic generation. - These estimates are saved directly into the output CSV.</p>"},{"location":"how-this-works/#data-organization","title":"Data Organization","text":"<p>The final output is a structured CSV file (managed via <code>polars</code>) containing metadata, file paths, and AI outputs. Corresponding files are organized into subdirectories within a central artifacts folder:</p> <pre><code>youtube-to-docs-artifacts/\n\u251c\u2500\u2500 youtube-docs.csv              # The main data file\n\u251c\u2500\u2500 transcript-files/             # Raw text transcripts (single long strings)\n\u251c\u2500\u2500 srt-files/                    # Standardized SRT transcript files\n\u251c\u2500\u2500 audio-files/                  # Extracted audio files (for AI transcription)\n\u251c\u2500\u2500 speaker-extraction-files/     # Identified speakers lists\n\u251c\u2500\u2500 qa-files/                     # Markdown Q&amp;A tables with timestamps\n\u251c\u2500\u2500 summary-files/                # Markdown summaries\n\u251c\u2500\u2500 one-sentence-summary-files/   # Concisely summarized content\n\u251c\u2500\u2500 tag-files/                    # AI-generated tags files\n\u251c\u2500\u2500 infographic-files/            # Generated infographic images\n\u251c\u2500\u2500 alt-text-files/               # Multimodal alt text for infographics\n\u2514\u2500\u2500 video-files/                  # Combined infographic + audio videos\n</code></pre> <p>This structure ensures that while the CSV provides a high-level data view, the actual content is easily accessible as standalone files.</p>"},{"location":"performance/","title":"Performance","text":"<p>This page displays the performance benchmarks for various models used in the summarization process.</p> model time (seconds) total_cost input_tokens output_tokens input_price_per_1m output_price_per_1m date (today) bedrock-nova-2-lite-v1 12.4 0.01 35828 1415 0.3 2.5 2026-01-04 bedrock-nova-pro-v1 14.14 0.03 37786 490 0.8 3.2 2026-01-04 gemini-3-flash-preview 15.08 0.02 37663 978 0.5 3.0 2026-01-04 bedrock-claude-haiku-4-5-20251001-v1 16.61 0.04 38457 1172 1.0 5.0 2026-01-04 foundry-gpt-5-mini 17.77 0.01 35905 1978 0.25 2.0 2026-01-04 bedrock-nova-premier-v1 19.22 0.1 37826 295 2.5 12.5 2026-01-04 bedrock-claude-opus-4-5-20251101-v1 25.59 0.22 38457 1037 5.0 25.0 2026-01-04 gemini-3-pro-preview 27.73 0.09 37663 1114 2.0 12.0 2026-01-04 vertex-claude-haiku-4-5@20251001 28.13 0.05 38457 2089 1.0 5.0 2026-01-04 vertex-gemini-2.5-flash 43.78 0.02 37662 2403 0.3 2.5 2026-01-04 vertex-gemini-2.5-pro 44.82 0.06 37662 1321 1.25 10.0 2026-01-04 vertex-claude-opus-4-5@20251101 49.52 0.23 38457 1650 5.0 25.0 2026-01-04 bedrock-claude-sonnet-4-5-20250929-v1 49.93 0.15 38457 2182 3.0 15.0 2026-01-04 vertex-claude-sonnet-4-5@20250929 98.23 0.16 38457 3041 3.0 15.0 2026-01-04 foundry-gpt-5-nano 999.0 0.0 0 0 0.0 0.0 2026-01-04 foundry-gpt-5 999.0 0.0 0 0 0.0 0.0 2026-01-04 foundry-gpt-5-pro 999.0 0.0 0 0 0.0 0.0 2026-01-04 foundry-gpt-5.2 999.0 0.0 0 0 0.0 0.0 2026-01-04 foundry-gpt-5.2-pro 999.0 0.0 0 0 0.0 0.0 2026-01-04"},{"location":"pricing/","title":"LLM Pricing","text":"<p>Compare the pricing of various Large Language Models. Prices are shown in USD per 1M tokens.</p> <p>[!NOTE] Prices last updated on 2026-02-06. All values are per 1 million tokens unless otherwise specified. required for non-token based models (marked with *):</p> <ul> <li>Audio/Character Pricing: Models priced by minute or character are converted to \"per 1M tokens\" assuming ~4 chars/token or ~200 tokens/minute. The estimated cost is split 50/50 between input and output for comparison.</li> </ul> <p></p> Reset Zoom Name \u21c5 Vendor \u21c5 Input ($/1M) \u21c5 Output ($/1M) \u21c5"},{"location":"usage/","title":"Usage Guide","text":"<p><code>youtube-to-docs</code> is a versatile tool designed to convert YouTube content into structured documentation, including transcripts, summaries, audio, and infographics. It is primarily designed as a Command Line Interface (CLI) tool but can also be used as a Python library.</p>"},{"location":"usage/#installation","title":"Installation","text":"<p>The recommended way to run <code>youtube-to-docs</code> is using <code>uvx</code>.</p>"},{"location":"usage/#basic-usage","title":"Basic Usage","text":"<p>For basic usage (YouTube data fetching, local CSV output, standard Gemini models):</p> <pre><code>uvx youtube-to-docs --help\n</code></pre>"},{"location":"usage/#optional-features-extras","title":"Optional Features (Extras)","text":"<p>To keep the installation footprint small, many features are optional. You can enable them by installing specific \"extras\":</p> Extra Description Dependencies <code>audio</code> Recommended. Required for downloading audio (for TTS/transcription). <code>yt-dlp</code> <code>video</code> Required for generating video files (combining audio &amp; infographic). <code>static-ffmpeg</code> <code>workspace</code> Required for saving to Google Drive. <code>google-api-python-client</code>, <code>google-auth-oauthlib</code> <code>m365</code> Required for saving to SharePoint/OneDrive. <code>msal</code>, <code>fastexcel</code>, <code>xlsxwriter</code>, <code>pypandoc</code> <code>aws</code> AWS Bedrock support. None <code>azure</code> Required if using Azure OpenAI models. <code>openai</code> <code>gcp</code> Required if using Google Gemini or Vertex AI models. <code>google-genai</code>, <code>google-cloud-speech</code>, <code>google-cloud-storage</code>, <code>google-cloud-texttospeech</code> <code>all</code> Installs all of the above. All optional dependencies. <p>How to use extras with <code>uvx</code>:</p> <p>Use the <code>--with</code> flag followed by the package name and extras in brackets.</p> <p>Example 1: Audio and Video support (Common) <pre><code>uvx --with \"youtube-to-docs[audio,video]\" youtube-to-docs ...\n</code></pre></p> <p>Example 2: Google Drive + Audio support <pre><code>uvx --with \"youtube-to-docs[workspace,audio]\" youtube-to-docs ...\n</code></pre></p> <p>Example 3: Everything (Full feature set) <pre><code>uvx --with \"youtube-to-docs[all]\" youtube-to-docs ...\n</code></pre></p>"},{"location":"usage/#installing-uv","title":"Installing <code>uv</code>","text":"<p>If you don't have <code>uv</code> installed, you can install it using the following commands:</p> <p>macOS/Linux: <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></p> <p>Windows: <pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre></p>"},{"location":"usage/#setup","title":"Setup","text":"<p>Before running the tool, ensure your environment is correctly configured with the necessary API keys and authentication files.</p>"},{"location":"usage/#1-environment-variables","title":"1. Environment Variables","text":"<p>Set the following environment variables based on the AI providers you intend to use.</p> Variable Description Required For <code>YOUTUBE_DATA_API_KEY</code> API key for the YouTube Data API v3. Fetching video metadata. <code>GEMINI_API_KEY</code> API key for Google Gemini models. Gemini models (<code>-m gemini...</code>). <code>PROJECT_ID</code> Google Cloud Project ID. GCP Vertex models (<code>-m vertex...</code>), GCP STT (<code>-t gcp...</code>) and GCP TTS (<code>--tts gcp...</code>). <code>YTD_GCS_BUCKET_NAME</code> Google Cloud Storage bucket name (write access). GCP STT models (<code>-t gcp...</code>) for temp audio storage. <code>AWS_BEARER_TOKEN_BEDROCK</code> AWS Bearer Token. AWS Bedrock models (<code>-m bedrock...</code>). <code>AZURE_FOUNDRY_ENDPOINT</code> Azure Foundry Endpoint URL. Azure Foundry models (<code>-m foundry...</code>). <code>AZURE_FOUNDRY_API_KEY</code> Azure Foundry API Key. Azure Foundry models (<code>-m foundry...</code>)."},{"location":"usage/#2-storage-authentication-optional","title":"2. Storage Authentication (Optional)","text":"<p>If you plan to save outputs to Google Drive (<code>workspace</code>) or Microsoft SharePoint/OneDrive (<code>sharepoint</code>), you need to configure authentication files in your home directory.</p>"},{"location":"usage/#google-drive-workspace","title":"Google Drive (Workspace)","text":"<p>Create a file at <code>~/.google_client_secret.json</code> with your Google Cloud OAuth 2.0 Client Secret JSON.</p> <p><pre><code>{\n  \"installed\": {\n    \"client_id\": \"YOUR_CLIENT_ID.apps.googleusercontent.com\",\n    \"project_id\": \"your-project-id\",\n    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n    \"client_secret\": \"YOUR_CLIENT_SECRET\",\n    \"redirect_uris\": [\"http://localhost\"]\n  }\n}\n</code></pre> *   First Run: The tool will open a browser window to authenticate and generate a <code>~/.google_client_token.json</code> file for future non-interactive use.</p>"},{"location":"usage/#microsoft-365-sharepointonedrive","title":"Microsoft 365 (SharePoint/OneDrive)","text":"<p>Create a file at <code>~/.azure_client.json</code> with your Azure App Registration details.</p> <p><pre><code>{\n    \"client_id\": \"YOUR_CLIENT_ID\",\n    \"authority\": \"https://login.microsoftonline.com/consumers\"\n}\n</code></pre> *   Authority: Use <code>.../consumers</code> for personal accounts or <code>.../YOUR_TENANT_ID</code> for organizational accounts. *   First Run: The tool will attempt to authenticate (silently or interactively) and cache the token in <code>~/.msal_token_cache.json</code>.</p>"},{"location":"usage/#command-line-interface-cli","title":"Command Line Interface (CLI)","text":"<p>The main command is <code>youtube-to-docs</code>.</p>"},{"location":"usage/#basic-usage_1","title":"Basic Usage","text":"<p>Running the command without arguments processes a default video:</p> <pre><code>youtube-to-docs\n</code></pre>"},{"location":"usage/#arguments","title":"Arguments","text":"Argument Description Default Example <code>video_id</code> The YouTube content to process. Can be a YouTube URL, Video ID, Playlist ID (starts with <code>PL</code>), Channel Handle (starts with <code>@</code>), or a comma-separated list of Video IDs. <code>atmGAHYpf_c</code> <code>youtube-to-docs @mychannel</code> <code>-o</code>, <code>--outfile</code> Path to save the output CSV file.  - Local path: <code>my-data.csv</code>  - Google Workspace: <code>workspace</code> or <code>w</code> (saves to Drive folder <code>youtube-to-docs-artifacts</code>) or a specific Folder ID.  - SharePoint/OneDrive: <code>sharepoint</code> or <code>s</code> (saves to <code>youtube-to-docs-artifacts</code>).  - No-op: <code>none</code> or <code>n</code> (skips saving to a file, results are printed to the console). <code>youtube-to-docs-artifacts/youtube-docs.csv</code> <code>-o n</code> <code>-t</code>, <code>--transcript</code> The transcript source to use. Can be <code>'youtube'</code> (default) to fetch existing YouTube transcripts, or an AI model name to perform STT on extracted audio (e.g. <code>gemini...</code> for Gemini API, <code>gcp-chirp3</code> for GCP Speech-to-Text V2). <code>youtube</code> <code>-t gemini-2.0-flash-exp</code> <code>-m</code>, <code>--model</code> The LLM(s) to use for speaker extraction, Q&amp;A generation, tag generation, and summarization. Supports models from Google (Gemini), Vertex AI, AWS Bedrock, and Azure Foundry. Can be a comma-separated list. <code>None</code> <code>-m gemini-3-flash-preview,vertex-claude-haiku-4-5@20251001</code> <code>--tts</code> The TTS model and voice to use for generating audio summaries. Format: <code>{model}-{voice}</code>. Supports Gemini models (e.g., <code>gemini-2.5-flash-preview-tts-Kore</code>) and GCP Cloud TTS (e.g., <code>gcp-chirp3-Kore</code>). <code>None</code> <code>--tts gcp-chirp3-Kore</code> <code>-i</code>, <code>--infographic</code> The image model to use for generating a visual summary. Supports models from Google (Gemini, Imagen), AWS Bedrock (Titan, Nova Canvas), and Azure Foundry. <code>None</code> <code>--infographic gemini-2.5-flash-image</code> <code>--alt-text-model</code> The LLM model to use for generating multimodal alt text for the infographic. Defaults to the summary model. <code>None</code> <code>--alt-text-model gemini-3-flash-preview</code> <code>-nys</code>, <code>--no-youtube-summary</code> If set, skips generating a secondary summary from the YouTube transcript when using an AI model for the primary transcript. <code>False</code> <code>--no-youtube-summary</code> <code>-l</code>, <code>--language</code> The target language(s) (e.g. 'es', 'fr', 'en'). Can be a comma-separated list. Default is 'en'. <code>en</code> <code>-l es,fr</code> <code>-cia</code>, <code>--combine-infographic-audio</code> Combine the infographic and audio summary into a video file (MP4). Requires both <code>--tts</code> and <code>--infographic</code> to be effective. <code>False</code> <code>--combine-infographic-audio</code> <code>--all</code> Shortcut to use a specific model suite for everything. Supported: <code>'gemini-flash'</code>, <code>'gemini-pro'</code>, <code>'gemini-flash-pro-image'</code>, <code>'gcp-pro'</code>. Sets models for summary, TTS, and infographic, and enables <code>--no-youtube-summary</code>. <code>None</code> <code>--all gemini-flash</code> <code>--verbose</code> Enable verbose output. <code>False</code> <code>--verbose</code>"},{"location":"usage/#examples","title":"Examples","text":"<p>1. Summarize the default video using a single model: <pre><code>youtube-to-docs -m gemini-3-flash-preview\n</code></pre></p> <p>2. Generate a transcript using Gemini 2.0 Flash and summarize: <pre><code>youtube-to-docs -t gemini-2.0-flash-exp -m gemini-3-flash-preview\n</code></pre></p> <p>3. Process the default video and save to a custom CSV: <pre><code>youtube-to-docs -o my-docs.csv\n</code></pre></p> <p>4. Summarize a Playlist using multiple models (Gemini and Vertex): <pre><code>youtube-to-docs PLGKTTEqwhiHHWO-jdxM1KtzTbWo6h0Ycl -m gemini-3-flash-preview,vertex-claude-haiku-4-5@20251001\n</code></pre></p> <p>5. Process a Channel with Summaries, TTS, and Infographics: <pre><code>youtube-to-docs @mga-othercommittees6625 -m vertex-claude-haiku-4-5@20251001 --tts gemini-2.5-flash-preview-tts-Kore --infographic gemini-2.5-flash-image\n</code></pre></p> <p>6. Generate an Infographic using AWS Bedrock: <pre><code>youtube-to-docs atmGAHYpf_c --infographic bedrock-titan-image-generator-v2:0\n</code></pre></p> <p>7. Create a Video (Infographic + Audio Summary): <pre><code>youtube-to-docs atmGAHYpf_c -m gemini-3-flash-preview --tts gemini-2.5-flash-preview-tts-Kore --infographic gemini-2.5-flash-image --combine-infographic-audio\n</code></pre></p>"},{"location":"usage/#csv-column-reference","title":"CSV Column Reference","text":"<p>The output CSV file contains a variety of columns depending on the arguments provided. Below is a reference of the possible columns:</p>"},{"location":"usage/#base-metadata","title":"Base Metadata","text":"<ul> <li>URL: The full YouTube video URL.</li> <li>Title: The title of the video.</li> <li>Description: The video description.</li> <li>Data Published: The date the video was published.</li> <li>Channel: The name of the YouTube channel.</li> <li>Tags: Video tags from YouTube (comma-separated).</li> <li>Tags {Transcript} {Model} model: Up to 5 AI-generated tags based on the transcript.</li> <li>Duration: The duration of the video.</li> <li>Transcript characters from youtube: The total number of characters in the YouTube transcript.</li> <li>Transcript characters from {model}: The total number of characters in the AI-generated transcript (if applicable).</li> <li>Audio File: Path to the extracted audio file (used for AI transcription).</li> </ul>"},{"location":"usage/#files","title":"Files","text":"<ul> <li>Transcript File {type}: Path to the saved transcript file. <code>{type}</code> is either <code>youtube generated</code>, <code>human generated</code>, or <code>{model} generated</code>. Suffix <code>(lang)</code> added for non-English.</li> <li>SRT File {type}: Path to the saved SRT transcript file. Standardized timestamps for accessibility and timing.</li> <li>Speakers File {model}: Path to the saved speaker extraction text file.</li> <li>QA File {model}: Path to the saved Q&amp;A Markdown file. Includes Timestamp and Timestamp URL (markdown hyperlink) columns. Suffix <code>(lang)</code> added for non-English.</li> <li>Summary File {model}: Path to the Markdown summary file generated by a specific model. Suffix <code>(lang)</code> added for non-English.</li> <li>One Sentence Summary File {model}: Path to the one-sentence summary file.</li> <li>Tags File {model}: Path to the AI-generated tags file.</li> <li>Summary Infographic File {model} {infographic_model}: Path to the generated infographic image.</li> <li>Summary Infographic Alt Text File {model} {infographic_model}: Path to the generated alt text file.</li> <li>Summary Audio File {model} {tts_model} File: Path to the generated TTS audio file. Suffix <code>(lang)</code> added for non-English.</li> <li>Video File: Path to the generated MP4 video combining the infographic and audio.</li> </ul>"},{"location":"usage/#ai-outputs-costs","title":"AI Outputs &amp; Costs","text":"<ul> <li>Speakers {model}: The extracted list of speakers and their roles.</li> <li>{normalized_model} Speaker extraction cost ($): The estimated API cost for speaker extraction.</li> <li>QA Text {model}: The full text of the Q&amp;A pairs (also saved to the Q&amp;A file).</li> <li>{normalized_model} QA cost ($): The estimated API cost for Q&amp;A generation.</li> <li>Summary Text {model}: The full text of the summary (also saved to the summary file).</li> <li>One Sentence Summary {model}: The one-sentence summary text.</li> <li>Summary Infographic Alt Text {model} {infographic_model}: The full multimodal alt text for the infographic.</li> <li>{normalized_model} summary cost ($): The total estimated API cost for both speaker extraction and summarization.</li> <li>Summary Infographic Cost {model} {infographic_model} ($): The estimated API cost for infographic generation.</li> <li>Summary Infographic Alt Text Cost {model} {infographic_model} ($): The estimated API cost for alt text generation.</li> <li>{normalized_model} tags cost from {transcript} ($): The estimated API cost for AI tag generation.</li> <li>{normalized_model} STT cost ($): The estimated API cost for Speech-to-Text generation.</li> </ul> <p>Note: <code>{normalized_model}</code> refers to the model name with prefixes (like <code>vertex-</code>) and date suffixes removed for cleaner column headers.</p>"},{"location":"usage/#speaker-extraction","title":"Speaker Extraction","text":"<p>When a model is specified using the <code>-m</code> or <code>--model</code> argument, the tool automatically performs speaker extraction before generating a summary.</p> <ul> <li>Model Matching: The extraction uses the same model as the summary. If multiple models are provided, each will perform its own extraction.</li> <li>Structured Output: It identifies speakers and their professional titles or roles (e.g., \"Speaker 1 (Senator Katie Fry Hester, Co-Chair)\").</li> <li>Cost Tracking: The cost of speaker extraction is tracked separately in the <code>{model} Speaker extraction cost ($)</code> column and included in the total <code>{model} summary cost ($)</code>.</li> <li>Unknowns: If a speaker or title cannot be identified, the tool uses the placeholder <code>UNKNOWN</code>. If no speakers are detected at all, the field is set to <code>NaN</code>.</li> </ul>"},{"location":"usage/#mcp-server","title":"MCP Server","text":"<p>This tool also functions as a Model Context Protocol (MCP) server, allowing it to be used as a tool by AI agents (like the Gemini CLI).</p> <p>The server exposes a <code>process_video</code> tool that mirrors the CLI functionality.</p>"},{"location":"usage/#configuration","title":"Configuration","text":"<p>The repository includes a <code>gemini-extension.json</code> file at the root, which configures the MCP server for use with the Gemini CLI.</p>"},{"location":"usage/#usage","title":"Usage","text":"<p>Once the extension is registered with your agent, you can ask it to process videos using natural language:</p> <p>\"Save a summary of https://www.youtube.com/watch?v=KuPc06JgI_A\"</p> <p>The agent will prompt you for any necessary details (like the model to use) and then execute the tool.</p>"},{"location":"usage/#install-as-a-gemini-cli-extension","title":"Install as a Gemini CLI extension","text":"<pre><code>gemini extensions install https://github.com/DoIT-Artificial-Intelligence/youtube-to-docs.git\n</code></pre>"},{"location":"usage/#library-usage","title":"Library Usage","text":"<p>While primarily a CLI, you can import core functions for custom workflows.</p> <pre><code>from youtube_to_docs.transcript import fetch_transcript\n\nvideo_id = \"atmGAHYpf_c\"\ntranscript, is_generated = fetch_transcript(video_id)\n</code></pre>"},{"location":"usage/#why-use-youtube-to-docs","title":"Why use <code>youtube-to-docs</code>?","text":"<p>You might find other tools that download YouTube transcripts, but <code>youtube-to-docs</code> distinguishes itself in several ways:</p> <ol> <li> <p>Multimodal Output: It doesn't just stop at text.</p> <ul> <li>Summaries: Uses state-of-the-art LLMs to create concise summaries.</li> <li>Speaker Extraction: Automatically identifies speakers and their titles/roles from the transcript.</li> <li>Audio (TTS): Converts summaries into audio files, perfect for listening on the go.</li> <li>Tags: Automatically generates up to 5 relevant tags for the video content.</li> <li>Visuals (Infographics): Generates AI-created infographics to visually represent the content.</li> <li>Timestamps &amp; SRT: Automatically generates <code>.srt</code> files and includes precision timestamps in Q&amp;A tables, cross-referencing YouTube's own timing data for pinpoint accuracy when using AI transcripts.</li> <li>Videos: Combines infographics and audio into a shareable video summary.</li> </ul> </li> <li> <p>Structured Data (CSV/Polars):</p> <ul> <li>Instead of loose files, metadata and paths are organized into a robust CSV file using <code>polars</code>.</li> <li>This makes it incredibly easy to import the data into Google Sheets, Excel, or a database for further analysis or publishing.</li> </ul> </li> <li> <p>Batch Processing:</p> <ul> <li>Seamlessly handles individual videos, entire playlists, or full channels with a single command.</li> </ul> </li> <li> <p>Multi-Provider Support:</p> <ul> <li>Agnostic to the LLM provider. Whether you use Google Gemini, Vertex AI, AWS Bedrock, or Azure Foundry, you can plug in your preferred model.</li> </ul> </li> <li> <p>Cost Awareness:</p> <ul> <li>When using paid API models, it tracks and estimates the cost of summarization, saving it directly to your data file.</li> </ul> </li> </ol>"},{"location":"assets/time_to_summarize/","title":"Time to summarize","text":"model time (seconds) total_cost input_tokens output_tokens input_price_per_1m output_price_per_1m date (today) bedrock-nova-2-lite-v1 12.4 0.01 35828 1415 0.3 2.5 2026-01-04 bedrock-nova-pro-v1 14.14 0.03 37786 490 0.8 3.2 2026-01-04 gemini-3-flash-preview 15.08 0.02 37663 978 0.5 3.0 2026-01-04 bedrock-claude-haiku-4-5-20251001-v1 16.61 0.04 38457 1172 1.0 5.0 2026-01-04 foundry-gpt-5-mini 17.77 0.01 35905 1978 0.25 2.0 2026-01-04 bedrock-nova-premier-v1 19.22 0.1 37826 295 2.5 12.5 2026-01-04 bedrock-claude-opus-4-5-20251101-v1 25.59 0.22 38457 1037 5.0 25.0 2026-01-04 gemini-3-pro-preview 27.73 0.09 37663 1114 2.0 12.0 2026-01-04 vertex-claude-haiku-4-5@20251001 28.13 0.05 38457 2089 1.0 5.0 2026-01-04 vertex-gemini-2.5-flash 43.78 0.02 37662 2403 0.3 2.5 2026-01-04 vertex-gemini-2.5-pro 44.82 0.06 37662 1321 1.25 10.0 2026-01-04 vertex-claude-opus-4-5@20251101 49.52 0.23 38457 1650 5.0 25.0 2026-01-04 bedrock-claude-sonnet-4-5-20250929-v1 49.93 0.15 38457 2182 3.0 15.0 2026-01-04 vertex-claude-sonnet-4-5@20250929 98.23 0.16 38457 3041 3.0 15.0 2026-01-04 foundry-gpt-5-nano 999.0 0.0 0 0 0.0 0.0 2026-01-04 foundry-gpt-5 999.0 0.0 0 0 0.0 0.0 2026-01-04 foundry-gpt-5-pro 999.0 0.0 0 0 0.0 0.0 2026-01-04 foundry-gpt-5.2 999.0 0.0 0 0 0.0 0.0 2026-01-04 foundry-gpt-5.2-pro 999.0 0.0 0 0 0.0 0.0 2026-01-04"}]}