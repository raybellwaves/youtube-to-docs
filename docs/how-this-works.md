# How This Works

`youtube-to-docs` automates the conversion of YouTube videos into structured documentation and multimodal assets. This document explains the technical workflow and core components of the system.

## System Architecture

The tool operates as a pipeline that ingests YouTube content, processes it through various AI models, and outputs structured data and files.

### 1. Input Resolution
The entry point (`youtube_to_docs/main.py`) accepts flexible inputs:
- **Video IDs**: Direct processing.
- **Playlists**: Resolves all video IDs within a playlist.
- **Channels**: Resolves a channel's "Uploads" playlist to process all their videos.

**Key Component**: `youtube_to_docs.transcript.resolve_video_ids` uses the YouTube Data API to fetch lists of videos when a Playlist or Channel is provided.

### 2. Metadata, Audio & Transcript Retrieval
Once video IDs are resolved, the system gathers essential data:

-   **Metadata**: Title, description, tags, publication date, and duration are fetched via the YouTube Data API.
-   **Audio Extraction**: If an AI model is requested for transcription (via the `-t` flag), `yt-dlp` and `ffmpeg` are used to extract high-quality audio (`.m4a`) from the video.
-   **Transcripts**:
    *   **YouTube Source**: By default, `youtube-transcript-api` fetches existing captions, prioritizing human-generated ones.
    *   **AI Source**: If specified, an AI model (like Gemini 2.0 Flash) processes the extracted audio file to generate a fresh, potentially higher-accuracy transcript.

> **Note on Auto-Captions**: Automatic captions are generated by speech recognition and may have accuracy issues with accents or background noise. They are not always immediately available after upload.

### 3. The LLM Pipeline
Text processing is handled by Large Language Models (LLMs) defined in `youtube_to_docs/llms.py`. The pipeline is model-agnostic, supporting Google Gemini, Vertex AI, AWS Bedrock, and Azure Foundry.

For each video, the specified model performs three distinct tasks:

1.  **Speaker Extraction**:
    *   **Input**: Full transcript.
    *   **Task**: Identify speakers and their professional titles/roles.
    *   **Output**: A structured list (e.g., "Speaker 1 (Host)").

2.  **Q&A Generation**:
    *   **Input**: Full transcript + Identified Speakers.
    *   **Task**: Extract key questions and answers discussed in the video.
    *   **Output**: A Markdown table with columns for Questioner, Question, Responder, and Answer.

3.  **Summarization**:
    *   **Input**: Full transcript + Video Metadata.
    *   **Task**: Create a concise, comprehensive summary of the content.
    *   **Output**: A Markdown-formatted summary.

4.  **Multi-Language Support**:
    *   The tool supports processing videos in multiple languages via the `--language` argument.
    *   It iterates through each requested language, fetching or generating transcripts, summaries, Q&A, and infographics for that specific language.
    *   File names and column headers are suffixed with the language code (e.g., `(es)`) to keep assets organized.

### 5. Multimodal Generation
Beyond text, the tool creates audio and visual assets:

- **Text-to-Speech (TTS)**:
    *   Uses models (like Gemini's TTS) to convert the generated summary into an audio file.
    *   This allows users to "listen" to the video summary.

- **Infographics**:
    *   Uses image generation models to create a visual representation of the summary.
    *   **Supported Providers**:
        *   **Google**: Gemini, Imagen.
        *   **AWS Bedrock**: Titan Image Generator, Nova Canvas (requires `AWS_BEARER_TOKEN_BEDROCK`).
        *   **Azure Foundry**: GPT Image models (requires `AZURE_FOUNDRY_ENDPOINT` and `AZURE_FOUNDRY_API_KEY`).
    *   The prompt includes the video title and the generated summary text to ensure relevance.

- **Video Generation**:
    *   Combines the generated infographic (visual) and TTS audio (sound) into a single MP4 video file.
    *   Uses `static-ffmpeg` to perform the merging, ensuring no external FFmpeg installation is required.
    *   This provides a shareable "video summary" format.

### 6. Cost Tracking
The system includes a pricing engine (`youtube_to_docs/prices.py`) that tracks token usage for every API call.
- It calculates costs for input and output tokens based on the specific model used.
- Costs are aggregated for speaker extraction, Q&A, summarization, and infographic generation.
- These estimates are saved directly into the output CSV.

## Data Organization

The final output is a structured CSV file (managed via `polars`) containing metadata, file paths, and AI outputs. Corresponding files are organized into subdirectories within a central artifacts folder:

```text
youtube-to-docs-artifacts/
├── youtube-docs.csv              # The main data file
├── transcript-files/             # Raw text transcripts
├── audio-files/                  # Extracted audio files (for AI transcription)
├── speaker-extraction-files/     # Identified speakers lists
├── qa-files/                     # Markdown Q&A tables
├── summary-files/                # Markdown summaries
├── infographic-files/            # Generated infographic images
└── video-files/                  # Combined infographic + audio videos
```

This structure ensures that while the CSV provides a high-level data view, the actual content is easily accessible as standalone files.