# How This Works

`youtube-to-docs` automates the conversion of YouTube videos into structured documentation and multimodal assets. This document explains the technical workflow and core components of the system.

## System Architecture

The tool operates as a pipeline that ingests YouTube content, processes it through various AI models, and outputs structured data and files.

### 1. Input Resolution
The entry point (`youtube_to_docs/main.py`) accepts flexible inputs:
- **Video IDs**: Direct processing.
- **Playlists**: Resolves all video IDs within a playlist.
- **Channels**: Resolves a channel's "Uploads" playlist to process all their videos.

**Key Component**: `youtube_to_docs.transcript.resolve_video_ids` uses the YouTube Data API to fetch lists of videos when a Playlist or Channel is provided.

### 2. Metadata & Transcript Retrieval
Once video IDs are resolved, the system gathers essential data:
- **Metadata**: Title, description, tags, publication date, and duration are fetched via the YouTube Data API.
- **Transcripts**: The `youtube-transcript-api` is used to download captions. It prioritizes human-generated captions but falls back to YouTube's auto-generated ones.

> **Note on Auto-Captions**: Automatic captions are generated by speech recognition and may have accuracy issues with accents or background noise. They are not always immediately available after upload.

### 3. The LLM Pipeline
Text processing is handled by Large Language Models (LLMs) defined in `youtube_to_docs/llms.py`. The pipeline is model-agnostic, supporting Google Gemini, Vertex AI, AWS Bedrock, and Azure Foundry.

For each video, the specified model performs three distinct tasks:

1.  **Speaker Extraction**:
    *   **Input**: Full transcript.
    *   **Task**: Identify speakers and their professional titles/roles.
    *   **Output**: A structured list (e.g., "Speaker 1 (Host)").

2.  **Q&A Generation**:
    *   **Input**: Full transcript + Identified Speakers.
    *   **Task**: Extract key questions and answers discussed in the video.
    *   **Output**: A Markdown table with columns for Questioner, Question, Responder, and Answer.

3.  **Summarization**:
    *   **Input**: Full transcript + Video Metadata.
    *   **Task**: Create a concise, comprehensive summary of the content.
    *   **Output**: A Markdown-formatted summary.

### 4. Multimodal Generation
Beyond text, the tool creates audio and visual assets:

- **Text-to-Speech (TTS)**:
    *   Uses models (like Gemini's TTS) to convert the generated summary into an audio file.
    *   This allows users to "listen" to the video summary.

- **Infographics**:
    *   Uses image generation models (like Imagen or Gemini) to create a visual representation of the summary.
    *   The prompt includes the video title and the generated summary text to ensure relevance.

### 5. Cost Tracking
The system includes a pricing engine (`youtube_to_docs/prices.py`) that tracks token usage for every API call.
- It calculates costs for input and output tokens based on the specific model used.
- Costs are aggregated for speaker extraction, Q&A, summarization, and infographic generation.
- These estimates are saved directly into the output CSV.

## Data Organization

The final output is a structured CSV file (managed via `polars`) containing metadata, file paths, and AI outputs. Corresponding files are organized into subdirectories:

```text
output_directory/
├── transcript-files/             # Raw text transcripts
├── speaker-extraction-files/     # Identified speakers lists
├── qa-files/                     # Markdown Q&A tables
├── summary-files/                # Markdown summaries
├── summary-audio-files/          # TTS audio files
└── infographic-files/            # Generated infographic images
```

This structure ensures that while the CSV provides a high-level data view, the actual content is easily accessible as standalone files.